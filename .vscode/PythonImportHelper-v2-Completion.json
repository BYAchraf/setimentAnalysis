[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Form",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "HTMLResponse",
        "importPath": "starlette.responses",
        "description": "starlette.responses",
        "isExtraImport": true,
        "detail": "starlette.responses",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tensorflow.keras.preprocessing.text",
        "description": "tensorflow.keras.preprocessing.text",
        "isExtraImport": true,
        "detail": "tensorflow.keras.preprocessing.text",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tensorflow.keras.preprocessing.text",
        "description": "tensorflow.keras.preprocessing.text",
        "isExtraImport": true,
        "detail": "tensorflow.keras.preprocessing.text",
        "documentation": {}
    },
    {
        "label": "pad_sequences",
        "importPath": "tensorflow.keras.preprocessing.sequence",
        "description": "tensorflow.keras.preprocessing.sequence",
        "isExtraImport": true,
        "detail": "tensorflow.keras.preprocessing.sequence",
        "documentation": {}
    },
    {
        "label": "pad_sequences",
        "importPath": "tensorflow.keras.preprocessing.sequence",
        "description": "tensorflow.keras.preprocessing.sequence",
        "isExtraImport": true,
        "detail": "tensorflow.keras.preprocessing.sequence",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras.models",
        "description": "tensorflow.keras.models",
        "isExtraImport": true,
        "detail": "tensorflow.keras.models",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LSTM",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "SpatialDropout1D",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"bin\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir] + os.environ.get(\"PATH\", \"\").split(os.pathsep))\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.8/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"bin\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir] + os.environ.get(\"PATH\", \"\").split(os.pathsep))\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.8/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir] + os.environ.get(\"PATH\", \"\").split(os.pathsep))\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.8/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"../lib/python3.8/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"../lib/python3.8/site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": "DeployEnv.bin.activate_this",
        "description": "DeployEnv.bin.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": "DeployEnv.bin.activate_this",
        "documentation": {}
    },
    {
        "label": "preProcess_data",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def preProcess_data(text): #cleaning the data\n    text = text.lower()\n    new_text = re.sub('[^a-zA-z0-9\\s]','',text)\n    new_text = re.sub('rt', '', new_text)\n    return new_text\napp = FastAPI()\ndata = pd.read_csv('archive/Sentiment.csv')\ntokenizer = Tokenizer(num_words=2000, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\ndef my_pipeline(text): #pipeline",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "my_pipeline",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def my_pipeline(text): #pipeline\n  text_new = preProcess_data(text)\n  X = tokenizer.texts_to_sequences(pd.Series(text_new).values)\n  X = pad_sequences(X, maxlen=28)\n  return X\n@app.get('/') #basic get view\ndef basic_view():\n    return {\"WELCOME\": \"GO TO /docs route, or /post or send post request to /predict \"}\n@app.get('/predict', response_class=HTMLResponse) #data input by forms\ndef take_inp():",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "basic_view",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def basic_view():\n    return {\"WELCOME\": \"GO TO /docs route, or /post or send post request to /predict \"}\n@app.get('/predict', response_class=HTMLResponse) #data input by forms\ndef take_inp():\n    return '''<form method=\"post\"> \n    <input type=\"text\" maxlength=\"28\" name=\"text\" value=\"Text Emotion to be tested\"/>  \n    <input type=\"submit\"/> \n    </form>'''\n@app.post('/predict') #prediction on data\ndef predict(text:str = Form(...)): #input is from forms",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "take_inp",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def take_inp():\n    return '''<form method=\"post\"> \n    <input type=\"text\" maxlength=\"28\" name=\"text\" value=\"Text Emotion to be tested\"/>  \n    <input type=\"submit\"/> \n    </form>'''\n@app.post('/predict') #prediction on data\ndef predict(text:str = Form(...)): #input is from forms\n    clean_text = my_pipeline(text) #cleaning and preprocessing of the texts\n    loaded_model = tf.keras.models.load_model('sentiment.h5') #loading the saved model\n    predictions = loaded_model.predict(clean_text) #making predictions",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def predict(text:str = Form(...)): #input is from forms\n    clean_text = my_pipeline(text) #cleaning and preprocessing of the texts\n    loaded_model = tf.keras.models.load_model('sentiment.h5') #loading the saved model\n    predictions = loaded_model.predict(clean_text) #making predictions\n    sentiment = int(np.argmax(predictions)) #index of maximum prediction\n    probability = max(predictions.tolist()[0]) #probability of maximum prediction\n    if sentiment==0: #assigning appropriate name to prediction\n        t_sentiment = 'negative'\n    elif sentiment==1:\n        t_sentiment = 'neutral'",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app = FastAPI()\ndata = pd.read_csv('archive/Sentiment.csv')\ntokenizer = Tokenizer(num_words=2000, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\ndef my_pipeline(text): #pipeline\n  text_new = preProcess_data(text)\n  X = tokenizer.texts_to_sequences(pd.Series(text_new).values)\n  X = pad_sequences(X, maxlen=28)\n  return X\n@app.get('/') #basic get view",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "data = pd.read_csv('archive/Sentiment.csv')\ntokenizer = Tokenizer(num_words=2000, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\ndef my_pipeline(text): #pipeline\n  text_new = preProcess_data(text)\n  X = tokenizer.texts_to_sequences(pd.Series(text_new).values)\n  X = pad_sequences(X, maxlen=28)\n  return X\n@app.get('/') #basic get view\ndef basic_view():",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "tokenizer = Tokenizer(num_words=2000, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\ndef my_pipeline(text): #pipeline\n  text_new = preProcess_data(text)\n  X = tokenizer.texts_to_sequences(pd.Series(text_new).values)\n  X = pad_sequences(X, maxlen=28)\n  return X\n@app.get('/') #basic get view\ndef basic_view():\n    return {\"WELCOME\": \"GO TO /docs route, or /post or send post request to /predict \"}",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "preProcess_data",
        "kind": 2,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "def preProcess_data(text):\n    text = text.lower()\n    new_text = re.sub('[^a-zA-z0-9\\s]','',text)\n    new_text = re.sub('rt', '', new_text)\n    return new_text\ndata['text'] = data['text'].apply(preProcess_data)\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "data = pd.read_csv('archive/Sentiment.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\ndef preProcess_data(text):\n    text = text.lower()\n    new_text = re.sub('[^a-zA-z0-9\\s]','',text)\n    new_text = re.sub('rt', '', new_text)\n    return new_text\ndata['text'] = data['text'].apply(preProcess_data)\nmax_fatures = 2000",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "data = data[['text','sentiment']]\ndef preProcess_data(text):\n    text = text.lower()\n    new_text = re.sub('[^a-zA-z0-9\\s]','',text)\n    new_text = re.sub('rt', '', new_text)\n    return new_text\ndata['text'] = data['text'].apply(preProcess_data)\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "data['text']",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "data['text'] = data['text'].apply(preProcess_data)\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X, 28) \nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\nembed_dim = 128\nlstm_out = 196",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "max_fatures",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X, 28) \nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\nembed_dim = 128\nlstm_out = 196\nmodel = Sequential()",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X, 28) \nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\nembed_dim = 128\nlstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "X = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X, 28) \nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\nembed_dim = 128\nlstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "X = pad_sequences(X, 28) \nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\nembed_dim = 128\nlstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(LSTM(128,recurrent_dropout=0.2))",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Y",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "Y = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\nembed_dim = 128\nlstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(LSTM(128,recurrent_dropout=0.2))\nmodel.add(Dense(3,activation='softmax'))",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "embed_dim",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "embed_dim = 128\nlstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(LSTM(128,recurrent_dropout=0.2))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nbatch_size = 512",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "lstm_out",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "lstm_out = 196\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(LSTM(128,recurrent_dropout=0.2))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nbatch_size = 512\nmodel.fit(X_train, Y_train, epochs = 50, batch_size=batch_size, validation_data=(X_test, Y_test))",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "model = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(LSTM(128,recurrent_dropout=0.2))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nbatch_size = 512\nmodel.fit(X_train, Y_train, epochs = 50, batch_size=batch_size, validation_data=(X_test, Y_test))\nmodel.save('sentiment.h5')",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "model.compile(loss",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nbatch_size = 512\nmodel.fit(X_train, Y_train, epochs = 50, batch_size=batch_size, validation_data=(X_test, Y_test))\nmodel.save('sentiment.h5')",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "batch_size = 512\nmodel.fit(X_train, Y_train, epochs = 50, batch_size=batch_size, validation_data=(X_test, Y_test))\nmodel.save('sentiment.h5')",
        "detail": "model",
        "documentation": {}
    }
]